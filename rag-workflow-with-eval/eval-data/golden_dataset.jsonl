{"question": "What was the key innovation in DeepSeek-Coder's training approach?", "answer": "The key innovation was a 'fill-in-the-middle' training objective, where the model is trained to predict a masked segment of code given the surrounding context from the beginning and end of the file."}
{"question": "How large is the DeepSeek-Coder model and what data was it trained on?", "answer": "The largest DeepSeek-Coder model has 33 billion parameters. It was trained on a dataset of 2 trillion tokens, comprising 80% code from GitHub and StackExchange, and 20% natural language text."}
{"question": "What is 'Group-wise Code Completion' and why is it used?", "answer": "Group-wise Code Completion is a method where the model completes an entire syntactic block of code at once, rather than predicting token-by-token. This approach respects the code's grammatical structure and improves the quality and relevance of the generated code."}
{"question": "On which benchmark did DeepSeek-Coder surpass GPT-4?", "answer": "DeepSeek-Coder 33B Instruct model surpassed the performance of GPT-4 on the HumanEval benchmark for code generation."}
{"question": "How does DeepSeek-Coder handle long context windows for code understanding?", "answer": "It uses a window-based attention mechanism, allowing it to efficiently handle context windows of up to 16,000 tokens, which is crucial for understanding entire files or complex codebases."}
